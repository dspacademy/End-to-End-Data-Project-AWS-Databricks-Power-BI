{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbfdb38e-d4fc-4187-89fa-e7867988872a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸš€ Mount AWS S3 Bucket to Databricks File System (DBFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d8f3eeb-e283-4370-aef1-c725377ba4e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-4802581997944924>, line 15\u001b[0m\n",
       "\u001b[1;32m     12\u001b[0m mnt_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
       "\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Mount the S3 bucket to DBFS using the s3a protocol and regional endpoint\u001b[39;00m\n",
       "\u001b[0;32m---> 15\u001b[0m dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mmount(\n",
       "\u001b[1;32m     16\u001b[0m   source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3a://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCESS_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENCODED_SECRET_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maws_bucket_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Authenticated S3 URI\u001b[39;00m\n",
       "\u001b[1;32m     17\u001b[0m   mount_point \u001b[38;5;241m=\u001b[39m mnt_name,  \u001b[38;5;66;03m# Local mount point in DBFS\u001b[39;00m\n",
       "\u001b[1;32m     18\u001b[0m   extra_configs \u001b[38;5;241m=\u001b[39m {\n",
       "\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.s3a.endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3.ca-central-1.amazonaws.com\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Specify AWS region endpoint\u001b[39;00m\n",
       "\u001b[1;32m     20\u001b[0m   }\n",
       "\u001b[1;32m     21\u001b[0m )\n",
       "\n",
       "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/dbutils.py:172\u001b[0m, in \u001b[0;36mprettify_exception_message.<locals>.f_with_exception_handling\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m    170\u001b[0m exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m    171\u001b[0m exc\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
       "\n",
       "\u001b[0;31mExecutionError\u001b[0m: An error occurred while calling o474.mount.\n",
       ": java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt; nested exception is: \n",
       "\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt\n",
       "\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.send0(BaseDbfsClient.scala:161)\n",
       "\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.sendIdempotent(BaseDbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1429)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1455)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:94)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:94)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:94)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:94)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:159)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1449)\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "Caused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt\n",
       "\tat scala.Predef$.require(Predef.scala:281)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:930)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1312)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:1085)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1301)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:938)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:136)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:40)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:52)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:51)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:51)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handleOtherRpc$3(DbfsServerBackend.scala:1110)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.handleOtherRpc(DbfsServerBackend.scala:1110)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$new$21(DbfsServerBackend.scala:357)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$new$21$adapted(DbfsServerBackend.scala:356)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler$.$anonfun$legacyWithRPCContext$1(UnaryRpcHandler.scala:512)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$callFunc$2(UnaryRpcHandler.scala:314)\n",
       "\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.callFunc(UnaryRpcHandler.scala:314)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFuncWithHooks(UnaryRpcHandler.scala:647)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.$anonfun$callFunc$3(UnaryRpcHandler.scala:625)\n",
       "\tat com.databricks.rpc.OperationSpan.$anonfun$wrapFuture$1(OperationSpan.scala:66)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.OperationSpan.withAttributionContext(OperationSpan.scala:20)\n",
       "\tat com.databricks.rpc.OperationSpan.wrapFuture(OperationSpan.scala:65)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFunc(UnaryRpcHandler.scala:623)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$3(UnaryRpcHandler.scala:274)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.withAttributionContext(UnaryRpcHandler.scala:43)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$2(UnaryRpcHandler.scala:234)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc0(UnaryRpcHandler.scala:234)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc$1(UnaryRpcHandler.scala:205)\n",
       "\tat com.databricks.rpc.armeria.server.internal.RequestCompletionTracker.wrap(RequestCompletionTracker.scala:184)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc(UnaryRpcHandler.scala:205)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleJettyRpc(UnaryRpcHandler.scala:88)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleJettyRpcWithAggregatedContent(UnaryRpcService.scala:522)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$2(UnaryRpcService.scala:433)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.withAttributionContext(UnaryRpcService.scala:187)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$1(UnaryRpcService.scala:428)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleContextAwareJettyRpcWithAggregatedContent(UnaryRpcService.scala:427)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$2(UnaryRpcService.scala:407)\n",
       "\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n",
       "\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:108)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$1(UnaryRpcService.scala:402)\n",
       "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
       "\tat scala.util.Success.map(Try.scala:213)\n",
       "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
       "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:46)\n",
       "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:46)\n",
       "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:77)\n",
       "\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n",
       "\tat grpc_shaded.com.linecorp.armeria.common.DefaultContextAwareRunnable.run(DefaultContextAwareRunnable.java:45)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)\n",
       "\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$2(InstrumentedExecutorService.scala:257)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$instrumentationWrapper$1(InstrumentedExecutorService.scala:299)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.threading.InstrumentedExecutorService.trackActiveThreads(InstrumentedExecutorService.scala:72)\n",
       "\tat com.databricks.threading.InstrumentedExecutorService.instrumentationWrapper(InstrumentedExecutorService.scala:287)\n",
       "\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$1(InstrumentedExecutorService.scala:259)\n",
       "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
       "\tat java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
       "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:840)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ExecutionError",
        "evalue": "An error occurred while calling o474.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt\n\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.send0(BaseDbfsClient.scala:161)\n\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.sendIdempotent(BaseDbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1429)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1455)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:94)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:94)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:94)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:94)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:159)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1449)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:930)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1312)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:1085)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1301)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:938)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:136)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:40)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:52)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:51)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:51)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handleOtherRpc$3(DbfsServerBackend.scala:1110)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.handleOtherRpc(DbfsServerBackend.scala:1110)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$new$21(DbfsServerBackend.scala:357)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$new$21$adapted(DbfsServerBackend.scala:356)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler$.$anonfun$legacyWithRPCContext$1(UnaryRpcHandler.scala:512)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$callFunc$2(UnaryRpcHandler.scala:314)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.callFunc(UnaryRpcHandler.scala:314)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFuncWithHooks(UnaryRpcHandler.scala:647)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.$anonfun$callFunc$3(UnaryRpcHandler.scala:625)\n\tat com.databricks.rpc.OperationSpan.$anonfun$wrapFuture$1(OperationSpan.scala:66)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.OperationSpan.withAttributionContext(OperationSpan.scala:20)\n\tat com.databricks.rpc.OperationSpan.wrapFuture(OperationSpan.scala:65)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFunc(UnaryRpcHandler.scala:623)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$3(UnaryRpcHandler.scala:274)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.withAttributionContext(UnaryRpcHandler.scala:43)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$2(UnaryRpcHandler.scala:234)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc0(UnaryRpcHandler.scala:234)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc$1(UnaryRpcHandler.scala:205)\n\tat com.databricks.rpc.armeria.server.internal.RequestCompletionTracker.wrap(RequestCompletionTracker.scala:184)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc(UnaryRpcHandler.scala:205)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleJettyRpc(UnaryRpcHandler.scala:88)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleJettyRpcWithAggregatedContent(UnaryRpcService.scala:522)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$2(UnaryRpcService.scala:433)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.withAttributionContext(UnaryRpcService.scala:187)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$1(UnaryRpcService.scala:428)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleContextAwareJettyRpcWithAggregatedContent(UnaryRpcService.scala:427)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$2(UnaryRpcService.scala:407)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:108)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$1(UnaryRpcService.scala:402)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:46)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:46)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:77)\n\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n\tat grpc_shaded.com.linecorp.armeria.common.DefaultContextAwareRunnable.run(DefaultContextAwareRunnable.java:45)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$2(InstrumentedExecutorService.scala:257)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$instrumentationWrapper$1(InstrumentedExecutorService.scala:299)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n\tat com.databricks.threading.InstrumentedExecutorService.trackActiveThreads(InstrumentedExecutorService.scala:72)\n\tat com.databricks.threading.InstrumentedExecutorService.instrumentationWrapper(InstrumentedExecutorService.scala:287)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$1(InstrumentedExecutorService.scala:259)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)\n"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)",
        "File \u001b[0;32m<command-4802581997944924>, line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m mnt_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Mount the S3 bucket to DBFS using the s3a protocol and regional endpoint\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mmount(\n\u001b[1;32m     16\u001b[0m   source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3a://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCESS_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENCODED_SECRET_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maws_bucket_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Authenticated S3 URI\u001b[39;00m\n\u001b[1;32m     17\u001b[0m   mount_point \u001b[38;5;241m=\u001b[39m mnt_name,  \u001b[38;5;66;03m# Local mount point in DBFS\u001b[39;00m\n\u001b[1;32m     18\u001b[0m   extra_configs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.s3a.endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3.ca-central-1.amazonaws.com\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Specify AWS region endpoint\u001b[39;00m\n\u001b[1;32m     20\u001b[0m   }\n\u001b[1;32m     21\u001b[0m )\n",
        "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/dbutils.py:172\u001b[0m, in \u001b[0;36mprettify_exception_message.<locals>.f_with_exception_handling\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    171\u001b[0m exc\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
        "\u001b[0;31mExecutionError\u001b[0m: An error occurred while calling o474.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt\n\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.send0(BaseDbfsClient.scala:161)\n\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.sendIdempotent(BaseDbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1429)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1455)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:94)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:94)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:94)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:94)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:159)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1449)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:930)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1312)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:1085)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1301)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:938)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:136)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:40)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:52)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:51)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:51)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handleOtherRpc$3(DbfsServerBackend.scala:1110)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.handleOtherRpc(DbfsServerBackend.scala:1110)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$new$21(DbfsServerBackend.scala:357)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$new$21$adapted(DbfsServerBackend.scala:356)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler$.$anonfun$legacyWithRPCContext$1(UnaryRpcHandler.scala:512)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$callFunc$2(UnaryRpcHandler.scala:314)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.callFunc(UnaryRpcHandler.scala:314)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFuncWithHooks(UnaryRpcHandler.scala:647)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.$anonfun$callFunc$3(UnaryRpcHandler.scala:625)\n\tat com.databricks.rpc.OperationSpan.$anonfun$wrapFuture$1(OperationSpan.scala:66)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.OperationSpan.withAttributionContext(OperationSpan.scala:20)\n\tat com.databricks.rpc.OperationSpan.wrapFuture(OperationSpan.scala:65)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFunc(UnaryRpcHandler.scala:623)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$3(UnaryRpcHandler.scala:274)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.withAttributionContext(UnaryRpcHandler.scala:43)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$2(UnaryRpcHandler.scala:234)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc0(UnaryRpcHandler.scala:234)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc$1(UnaryRpcHandler.scala:205)\n\tat com.databricks.rpc.armeria.server.internal.RequestCompletionTracker.wrap(RequestCompletionTracker.scala:184)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc(UnaryRpcHandler.scala:205)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleJettyRpc(UnaryRpcHandler.scala:88)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleJettyRpcWithAggregatedContent(UnaryRpcService.scala:522)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$2(UnaryRpcService.scala:433)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.withAttributionContext(UnaryRpcService.scala:187)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$1(UnaryRpcService.scala:428)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleContextAwareJettyRpcWithAggregatedContent(UnaryRpcService.scala:427)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$2(UnaryRpcService.scala:407)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:108)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$1(UnaryRpcService.scala:402)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:46)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:46)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:77)\n\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n\tat grpc_shaded.com.linecorp.armeria.common.DefaultContextAwareRunnable.run(DefaultContextAwareRunnable.java:45)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$2(InstrumentedExecutorService.scala:257)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$instrumentationWrapper$1(InstrumentedExecutorService.scala:299)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n\tat com.databricks.threading.InstrumentedExecutorService.trackActiveThreads(InstrumentedExecutorService.scala:72)\n\tat com.databricks.threading.InstrumentedExecutorService.instrumentationWrapper(InstrumentedExecutorService.scala:287)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$1(InstrumentedExecutorService.scala:259)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import urllib.parse  # Used to safely encode the secret key for URL usage\n",
    "\n",
    "# AWS credentials (use secrets management in production for security)\n",
    "ACCESS_KEY = \"<access key>\"\n",
    "SECRET_KEY = \"<secret_key>\"\n",
    "\n",
    "# URL-encode the secret key to handle special characters\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(SECRET_KEY, safe='')\n",
    "\n",
    "# Define the S3 bucket name and the DBFS mount point\n",
    "aws_bucket_name = \"taxi-weather-analytics-s3-bucket\"\n",
    "mnt_name = \"/mnt\"\n",
    "\n",
    "# Mount the S3 bucket to DBFS using the s3a protocol and regional endpoint\n",
    "dbutils.fs.mount(\n",
    "  source = f\"s3a://{ACCESS_KEY}:{ENCODED_SECRET_KEY}@{aws_bucket_name}\",  # Authenticated S3 URI\n",
    "  mount_point = mnt_name,  # Local mount point in DBFS\n",
    "  extra_configs = {\n",
    "    \"fs.s3a.endpoint\": \"s3.ca-central-1.amazonaws.com\"  # Specify AWS region endpoint\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09afa20c-23d9-4529-a967-1d8e7fedf646",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"path\":215},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761665228971}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List files inside your mounted bucket path\n",
    "display(dbutils.fs.ls(\"/mnt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb0eb085-644a-4560-a484-a43d96c10e51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Deal with NYC taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2349dab-03de-47bb-bf3b-88d979503639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ› ï¸ Load NYC Taxi Data and Register Bronze Delta Table in Hive Metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e2d62c-518c-4ded-8780-f0d737e32aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mnt_name = \"/mnt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd45a0f4-3af1-4571-b755-9b832b32f18a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"year\", \"\", \"Enter Year (e.g. 2025)\")\n",
    "dbutils.widgets.text(\"month\", \"\", \"Enter Month (e.g. 07)\")\n",
    "\n",
    "year = dbutils.widgets.get(\"year\")\n",
    "month = dbutils.widgets.get(\"month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc540715-f958-4514-ba7f-54898d9a6b30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b6ef4c-5fc2-4ccf-9ce6-d5f9ac540b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# year = \"2025\"\n",
    "# month = \"08\"\n",
    "\n",
    "input_filename = f\"yellow_tripdata_{year}-{month}.parquet\"\n",
    "input_path = f\"{mnt_name}/landing/nyc/taxi/{year}/{month}/{input_filename}\"\n",
    "target_path = \"/mnt/bronze/nyc/taxi_partitioned\"\n",
    "# Read July 2025 yellow taxi trip data from the mounted S3 landing zone\n",
    "df_taxi = spark.read.parquet(input_path)\n",
    "\n",
    "df_taxi = df_taxi.withColumn(\"year\", F.lit(year)) \\\n",
    "                 .withColumn(\"month\", F.lit(month))\n",
    "\n",
    "# Create the bronze_nyc schema in the Hive metastore if it doesn't already exist\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS hive_metastore.bronze_nyc_updated\")\n",
    "\n",
    "# Define the target path for saving the Delta table in the bronze zone\n",
    "# target_path = \"/mnt/bronze/nyc/taxi/2025/07/daily_trip_2025-07\"\n",
    "\n",
    "# Write the DataFrame as a Delta table to the bronze path, overwriting if it exists\n",
    "df_taxi.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .save(target_path)\n",
    "\n",
    "# Register the Delta table in the Hive metastore for SQL access\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS hive_metastore.bronze_nyc_updated.daily_trip\n",
    "    USING DELTA\n",
    "    LOCATION '{target_path}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3365b3d-e6fd-46a3-929c-ddf5d28d3032",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762989213171}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM hive_metastore.bronze_nyc_updated.daily_trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6b73434-9826-4fe4-9a2f-98710ffcada5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT YEAR(tpep_pickup_datetime) as year\n",
    ", month(tpep_pickup_datetime) as month\n",
    ", COUNT(*) as trip_count\n",
    "FROM hive_metastore.bronze_nyc_updated.daily_trip\n",
    "GROUP BY 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2d8644-c765-43ed-a30f-5d43ca58d800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- DROP TABLE hive_metastore.bronze_nyc_updated.daily_trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7fe1a4-b51e-4a0e-8957-5419c782b63d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- DROP SCHEMA hive_metastore.bronze_nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f3885d4-48fa-4e32-8bda-4f5b9d81530d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ðŸ“Š Query all records from the bronze-level NYC daily taxi trip Delta table\n",
    "SELECT * FROM hive_metastore.bronze_nyc.daily_trip;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a431677f-affb-46aa-bf27-0d1f9c06ee8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âœ¨ Clean and Standardize NYC Taxi Data for Silver Layer Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba5c3e6b-97d9-40be-afe4-944084d502d9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762989649474}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, to_date\n",
    "\n",
    "# Apply type casting and timestamp normalization to prepare data for analytics and downstream modeling\n",
    "taxi_silver = (\n",
    "    df_taxi\n",
    "    .withColumn(\"tpep_pickup_datetime\", to_timestamp(\"tpep_pickup_datetime\"))  # Convert pickup time to timestamp\n",
    "    .withColumn(\"tpep_dropoff_datetime\", to_timestamp(\"tpep_dropoff_datetime\"))  # Convert dropoff time to timestamp\n",
    "    .withColumn(\"passenger_count\", col(\"passenger_count\").cast(\"int\"))  # Ensure passenger count is integer\n",
    "    .withColumn(\"trip_distance\", col(\"trip_distance\").cast(\"double\"))  # Cast trip distance to double\n",
    "    .withColumn(\"fare_amount\", col(\"fare_amount\").cast(\"double\"))  # Cast fare amount to double\n",
    "    .withColumn(\"tip_amount\", col(\"tip_amount\").cast(\"double\"))  # Cast tip amount to double\n",
    "    .withColumn(\"total_amount\", col(\"total_amount\").cast(\"double\"))  # Cast total amount to double\n",
    "    .withColumn(\"date\", to_date(col(\"tpep_pickup_datetime\")))  # Extract date for partitioning or filtering\n",
    "    .withColumn(\"pickup_datetime\", col(\"tpep_pickup_datetime\").cast(\"timestamp\"))  # Duplicate pickup time for clarity\n",
    ")\n",
    "\n",
    "# Display the transformed silver-level DataFrame for inspection\n",
    "display(taxi_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81aeecae-4a4f-429b-b1fe-2da17c3652e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check count before cleaning\n",
    "taxi_silver.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6370a836-bdea-48b4-b221-21cfbef0c4cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean taxi data\n",
    "df_taxi_silver_clean = taxi_silver \\\n",
    "  .dropDuplicates()\n",
    "# Check count after removing duplicate\n",
    "df_taxi_silver_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dced23f-c763-4a9b-a875-251bc70b96c5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762989672802}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the cleaned taxi data\n",
    "display(df_taxi_silver_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1d975b9-2dc9-4c06-be82-829bee02563d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ§¼ Persist Cleaned NYC Taxi Data to Silver Layer and Register Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1d478e2-e5d4-49d6-a55b-619215f75e2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_register_delta_table(df, target_path, schema_name, table_name):\n",
    "    df = df.withColumn(\"year\", F.lit(year)) \\\n",
    "                 .withColumn(\"month\", F.lit(month))\n",
    "\n",
    "    # Create the silver_nyc schema in the Hive metastore if it doesn't already exist\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS hive_metastore.{schema_name}\")\n",
    "\n",
    "    # Define the target path for saving the cleaned Delta table in the silver zone\n",
    "    # target_path = \"/mnt/silver/nyc/taxi/2025/07/cleaned_daily_trip_2025-07\"\n",
    "\n",
    "    # Write the cleaned DataFrame to the silver path as a Delta table, overwriting if it exists\n",
    "    df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .partitionBy(\"year\", \"month\") \\\n",
    "        .save(target_path)\n",
    "\n",
    "    # Register the cleaned Delta table in the Hive metastore for SQL access\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS hive_metastore.{schema_name}.{table_name}\n",
    "        USING DELTA\n",
    "        LOCATION '{target_path}'\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d3e7b68-3385-4a6d-880e-6aa84d975abe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_register_delta_table(\n",
    "    df=df_taxi_silver_clean,\n",
    "    target_path=\"/mnt/silver/nyc/taxi_partitioned\",\n",
    "    schema_name=\"silver_nyc_updated\",\n",
    "    table_name=\"cleaned_daily_trip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07df8359-d23c-40a0-bfe9-05448174a05f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- DROP SCHEMA hive_metastore.silver_nyc_updated CASCADE;\n",
    "    \n",
    "-- Create silver taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c97382-a88c-4656-9f8b-dd6b5560580a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762990691373}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Display cleaned taxi data\n",
    "SELECT * FROM hive_metastore.silver_nyc_updated.cleaned_daily_trip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a351da4c-3b44-4ce5-aa7f-fec260dbaa64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare silver taxi trip data for gold layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "108e3a86-70b2-409f-94bc-4b30eeb9d16d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count, sum\n",
    "\n",
    "# Taxi daily summary\n",
    "df_taxi_silver_agg = df_taxi_silver_clean \\\n",
    "    .groupBy(\"date\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"trip_count\"), \n",
    "        avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "        sum(\"total_amount\").alias(\"total_revenue\")\n",
    "    )\n",
    "\n",
    "display(df_taxi_silver_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c30b8fa0-7bea-4632-a1e1-2e5da0ff61a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deal with Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca593c0e-54a3-4bed-bc58-103064900ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ðŸ“‚ List all top-level directories and files under the mounted S3 path (/mnt)\n",
    "display(dbutils.fs.ls(mnt_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ababdc6e-9bb9-45b5-bb38-757911b31511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingest weather data and persist the object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d3cd230-d11d-4b2f-b35f-04bed773afed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸŒ¦ï¸ Define Nested Schema for Daily Weather JSON Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f298ed-f760-4bd2-b5c1-f21941b224f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    "\n",
    "# Create a structured schema to parse weather JSON data with nested fields and arrays\n",
    "w_schema = StructType([\n",
    "    StructField(\"latitude\", DoubleType(), True),  # Geographic latitude of the location\n",
    "    StructField(\"longitude\", DoubleType(), True),  # Geographic longitude of the location\n",
    "    StructField(\"generationtime_ms\", DoubleType(), True),  # Time taken to generate the response (ms)\n",
    "    StructField(\"utc_offset_seconds\", IntegerType(), True),  # UTC offset in seconds\n",
    "    StructField(\"timezone\", StringType(), True),  # Full timezone name (e.g., America/New_York)\n",
    "    StructField(\"timezone_abbreviation\", StringType(), True),  # Abbreviated timezone (e.g., EST)\n",
    "    StructField(\"elevation\", DoubleType(), True),  # Elevation of the location in meters\n",
    "\n",
    "    # Units for each daily weather metric (e.g., Â°C, mm)\n",
    "    StructField(\"daily_units\", StructType([\n",
    "        StructField(\"time\", StringType(), True),\n",
    "        StructField(\"temperature_2m_max\", StringType(), True),\n",
    "        StructField(\"temperature_2m_min\", StringType(), True),\n",
    "        StructField(\"precipitation_sum\", StringType(), True)\n",
    "    ]), True),\n",
    "\n",
    "    # Actual daily weather data as arrays\n",
    "    StructField(\"daily\", StructType([\n",
    "        StructField(\"time\", ArrayType(StringType()), True),  # Dates for each observation\n",
    "        StructField(\"temperature_2m_max\", ArrayType(DoubleType()), True),  # Daily max temperatures\n",
    "        StructField(\"temperature_2m_min\", ArrayType(DoubleType()), True),  # Daily min temperatures\n",
    "        StructField(\"precipitation_sum\", ArrayType(DoubleType()), True)  # Daily total precipitation\n",
    "    ]), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f50f24a1-c554-479a-94b7-75076acdc371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸŒ¤ï¸ Load Raw Daily Weather JSON into DataFrame Using Defined Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6cc693f-d403-4c64-a368-1cedaa9873dd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760223324340}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read weather data from the mounted S3 landing path using the nested schema for proper parsing\n",
    "weather_df = (\n",
    "    spark.read\n",
    "         .schema(w_schema)\n",
    "         .json(f\"{mnt_name}/landing/nyc/weather/{year}/{month}/\")\n",
    ")\n",
    "\n",
    "# Display the parsed weather DataFrame for inspection and validation\n",
    "display(weather_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ce900f-42cf-4108-83e0-a1dbc0fbe6ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸŒ§ï¸ Flatten and Categorize Daily Weather Data for Silver Layer Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e350296-51bb-44a1-bce5-12e3554d907b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, arrays_zip, col, when\n",
    "\n",
    "# Step 1: Zip the daily weather arrays into a single array of structs for row-wise expansion\n",
    "zipped_df = weather_df.select(\n",
    "    \"latitude\", \"longitude\", \"generationtime_ms\", \"utc_offset_seconds\",\n",
    "    \"timezone\", \"timezone_abbreviation\", \"elevation\",\n",
    "    col(\"daily_units.*\"),  # Flatten the daily_units struct into individual columns\n",
    "    explode(arrays_zip(  # Combine daily arrays into a single array of structs and explode into rows\n",
    "        col(\"daily.time\"),\n",
    "        col(\"daily.temperature_2m_max\"),\n",
    "        col(\"daily.temperature_2m_min\"),\n",
    "        col(\"daily.precipitation_sum\")\n",
    "    )).alias(\"daily\")  # Alias the exploded struct as 'daily'\n",
    ")\n",
    "\n",
    "# Step 2: Flatten the zipped struct into individual columns for analysis and joining\n",
    "bronze_flat_df = zipped_df.select(\n",
    "    \"latitude\", \"longitude\", \"generationtime_ms\", \"utc_offset_seconds\",\n",
    "    \"timezone\", \"timezone_abbreviation\", \"elevation\",\n",
    "    col(\"time\").alias(\"unit_time_format\"),  # Units metadata for time\n",
    "    col(\"temperature_2m_max\").alias(\"unit_temp_max\"),  # Units metadata for max temp\n",
    "    col(\"temperature_2m_min\").alias(\"unit_temp_min\"),  # Units metadata for min temp\n",
    "    col(\"precipitation_sum\").alias(\"unit_precip_mm\"),  # Units metadata for precipitation\n",
    "    col(\"daily.time\").alias(\"date\"),  # Actual date of observation\n",
    "    col(\"daily.temperature_2m_max\").alias(\"temperature_max\"),  # Max temperature value\n",
    "    col(\"daily.temperature_2m_min\").alias(\"temperature_min\"),  # Min temperature value\n",
    "    col(\"daily.precipitation_sum\").alias(\"precipitation_mm\")  # Precipitation value\n",
    ")\n",
    "\n",
    "# Step 3: Add a rainfall category column based on precipitation thresholds\n",
    "silver_cleaned_df = bronze_flat_df.withColumn(\n",
    "    \"rainfall_category\",\n",
    "    when(col(\"precipitation_mm\") < 2, \"Dry/No rain\")\n",
    "    .when(col(\"precipitation_mm\") < 10, \"Light Rain\")\n",
    "    .when(col(\"precipitation_mm\") < 30, \"Moderate Rain\")\n",
    "    .when(col(\"precipitation_mm\") < 60, \"Heavy Rain\")\n",
    "    .when(col(\"precipitation_mm\") < 100, \"Very Heavy Rain\")\n",
    "    .otherwise(\"Extreme Rainfall\")\n",
    ")\n",
    "\n",
    "# Display the cleaned and categorized silver-level weather DataFrame\n",
    "silver_cleaned_df.display(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "133e1070-f781-4a16-a7ba-99f6d3ac406c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸŒŸ Enrich Aggregated Taxi Data with Daily Weather Metrics and Rainfall Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86ba17f0-6692-482a-8c87-2a706ed9da7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select relevant weather columns for joining with taxi data by date\n",
    "flat_df_selection = silver_cleaned_df.select(\n",
    "    \"date\",  # Date of weather observation\n",
    "    \"temperature_max\",  # Daily maximum temperature\n",
    "    \"temperature_min\",  # Daily minimum temperature\n",
    "    \"precipitation_mm\",  # Total daily precipitation in mm\n",
    "    \"rainfall_category\"  # Categorized rainfall intensity\n",
    ")\n",
    "\n",
    "# Perform a left join to enrich taxi aggregates with weather data based on matching date\n",
    "df_taxi_gold_updated = df_taxi_silver_agg.join(flat_df_selection, \"date\", \"left\")\n",
    "\n",
    "# Display the enriched gold-level DataFrame for inspection and downstream analysis\n",
    "display(df_taxi_gold_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4acc31b0-8691-4f82-9748-fb0e1560fd8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ† Persist Final Gold-Level Taxi + Weather Dataset and Register Delta Table in Hive Metastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abaee3ec-9043-4930-9e23-0f8967860bc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ§© Reusable Function to Save DataFrame as Delta Table and Register in Hive Metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aef41a17-9c54-4e2f-b040-dc95ff818826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_and_register_delta_table(df, target_path, schema_name, table_name):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame to the specified path as a Delta table and registers it in the Hive metastore.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The Spark DataFrame to persist.\n",
    "    - target_path (str): The DBFS or S3 path where the Delta table will be saved.\n",
    "    - schema_name (str): Hive metastore schema to create/use.\n",
    "    - table_name (str): Name of the table to register.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Create schema if it doesn't exist\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS hive_metastore.{schema_name}\")\n",
    "\n",
    "    # Write DataFrame as Delta table\n",
    "    df.write \\\n",
    "      .format(\"delta\") \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save(target_path)\n",
    "\n",
    "    # Register table in Hive metastore\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS hive_metastore.{schema_name}.{table_name}\n",
    "        USING DELTA\n",
    "        LOCATION '{target_path}'\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aac7ad9-2999-4c7c-bfdf-a626168c8c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save_register_delta_table(\n",
    "#     df=df_taxi_silver_clean,\n",
    "#     target_path=\"/mnt/silver/nyc/taxi_partitioned\",\n",
    "#     schema_name=\"silver_nyc_updated\",\n",
    "#     table_name=\"cleaned_daily_trip\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bf66a0a-dc7d-4e37-baa5-1f7e86e9bd50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_register_delta_table(\n",
    "    df=bronze_flat_df,\n",
    "    target_path=\"/mnt/bronze/nyc/weather_partitioned\",\n",
    "    schema_name=\"bronze_nyc_updated\",\n",
    "    table_name=\"daily_weather\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb65761e-e21f-4589-9947-ad988558c60c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_register_delta_table(\n",
    "    df=silver_cleaned_df,\n",
    "    target_path=\"/mnt/silver/nyc/weather_partitioned\",\n",
    "    schema_name=\"silver_nyc_updated\",\n",
    "    table_name=\"cleaned_daily_weather\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71ece20a-c2ea-4d91-abc0-c486d05cd9ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_register_delta_table(\n",
    "    df=df_taxi_gold_updated,\n",
    "    target_path=\"/mnt/gold/nyc/trip_weather_partitioned\",\n",
    "    schema_name=\"gold_nyc_updated\",\n",
    "    table_name=\"final_daily_trip_weather\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bfd70f5-94b8-43d1-a96a-409befddb4a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_taxi_gold_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6212deb2-b607-4f7e-b574-7e5275fb633c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762991185783}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View final gold data\n",
    "SELECT * FROM hive_metastore.gold_nyc_updated.final_daily_trip_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "294d0ca4-3e60-4159-be76-528de1680bd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Write data to Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bde5fc6-18a8-4f4b-90e5-7c7ede46c3f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ”— Test Redshift JDBC Connection from Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3bdbfd5-49e6-42d6-a684-3ba9d07e1267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # JDBC connection parameters\n",
    "# jdbc_url = \"jdbc:redshift://<add path>\"\n",
    "# user = \"<add user>\"\n",
    "# password = \"<add password>\"\n",
    "\n",
    "# # Use a lightweight system table for testing the connection\n",
    "# table_name = \"pg_catalog.pg_tables\"  # Redshift system catalog table\n",
    "\n",
    "# # -----------------------------------------------\n",
    "# # ðŸš€ Attempt to connect and read sample data\n",
    "# # -----------------------------------------------\n",
    "# try:\n",
    "#     # Read the system table using Spark JDBC\n",
    "#     test_df = spark.read \\\n",
    "#         .format(\"jdbc\") \\\n",
    "#         .option(\"url\", jdbc_url) \\\n",
    "#         .option(\"dbtable\", table_name) \\\n",
    "#         .option(\"user\", user) \\\n",
    "#         .option(\"password\", password) \\\n",
    "#         .option(\"driver\", \"com.amazon.redshift.jdbc42.Driver\") \\\n",
    "#         .load()\n",
    "\n",
    "#     # Display a few rows to confirm success\n",
    "#     print(\"âœ… Redshift connection successful. Sample rows:\")\n",
    "#     test_df.show(5)\n",
    "\n",
    "# except Exception as e:\n",
    "#     # Handle connection or read errors\n",
    "#     print(\"âŒ Redshift connection failed.\")\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80a8056-e0b9-44f0-837a-651555caba9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# redshift_url = \"jdbc:redshift://<add path>\"\n",
    "# redshift_table = \"gold_nyc_2.final_daily_trip_weather\"\n",
    "# redshift_properties = {\n",
    "#     \"user\": \"<add user>\",\n",
    "#     \"password\": \"<add password>\",\n",
    "#     \"driver\": \"com.amazon.redshift.jdbc.Driver\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2614f17a-94d6-4ebe-8220-9cc9d11494dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe\n",
    "# df_taxi_gold_updated.write \\\n",
    "#   .mode(\"append\") \\\n",
    "#   .jdbc(url=redshift_url, table=redshift_table, properties=redshift_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00fba3b0-ddc3-4ade-ab7e-7d8ee5290f3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # table\n",
    "# df_table = spark.table(\"nyc_weather_table\")\n",
    "\n",
    "# df_table.write \\\n",
    "#   .mode(\"append\") \\\n",
    "#   .jdbc(url=redshift_url, table=redshift_table, properties=redshift_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7edc187-6864-4388-85e4-70035a9405da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Schema and Table via JDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b11c6c8d-8c19-4445-a700-43e6a3cf4588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # SQL to create a new database and schema\n",
    "# create_schema_sql = \"CREATE SCHEMA IF NOT EXISTS gold_nyc_2;\"\n",
    "# create_table_sql = \"\"\"\n",
    "# CREATE TABLE gold_nyc_2.final_daily_trip_weather (\n",
    "#     date DATE,\n",
    "#     trip_count BIGINT,\n",
    "#     avg_fare DOUBLE PRECISION,\n",
    "#     total_revenue DOUBLE PRECISION,\n",
    "#     temperature_max DOUBLE PRECISION,\n",
    "#     temperature_min DOUBLE PRECISION,\n",
    "#     precipitation_mm DOUBLE PRECISION,\n",
    "#     rainfall_category VARCHAR(50)\n",
    "# );\n",
    "# \"\"\"\n",
    "\n",
    "# # Execute SQL via JVM bridge\n",
    "# conn = SparkSession.getActiveSession()._sc._jvm.java.sql.DriverManager.getConnection(\n",
    "#     redshift_url,\n",
    "#     redshift_properties[\"user\"],\n",
    "#     redshift_properties[\"password\"]\n",
    "# )\n",
    "# stmt = conn.createStatement()\n",
    "# stmt.execute(create_schema_sql)\n",
    "# stmt.execute(create_table_sql)\n",
    "# stmt.close()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91699b5c-6337-42e6-a581-55f44131bff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "redshift_url = \"jdbc:redshift://<add path>\"\n",
    "redshift_table = \"gold_nyc_updated.final_daily_trip_weather\"\n",
    "redshift_properties = {\n",
    "    \"user\": \"<add user>\",\n",
    "    \"password\": \"<add password>\",\n",
    "    \"driver\": \"com.amazon.redshift.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e324d775-be15-4c09-b84e-06e8c81e3685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# # JDBC connection to Redshift's default database (e.g., dev)\n",
    "# redshift_url = \"jdbc:redshift://<add path>\"\n",
    "# redshift_properties = {\n",
    "#     \"user\": \"<add user>\",\n",
    "#     \"password\": \"<add password>\",\n",
    "#     \"driver\": \"com.amazon.redshift.jdbc.Driver\"\n",
    "# }\n",
    "\n",
    "# SQL to create a new database and schema\n",
    "# create_schema_sql = \"CREATE SCHEMA IF NOT EXISTS gold_nyc_updated;\"\n",
    "# create_table_sql = \"\"\"\n",
    "# CREATE TABLE gold_nyc_updated.final_daily_trip_weather (\n",
    "#     date DATE,\n",
    "#     trip_count BIGINT,\n",
    "#     avg_fare DOUBLE PRECISION,\n",
    "#     total_revenue DOUBLE PRECISION,\n",
    "#     temperature_max DOUBLE PRECISION,\n",
    "#     temperature_min DOUBLE PRECISION,\n",
    "#     precipitation_mm DOUBLE PRECISION,\n",
    "#     rainfall_category VARCHAR(50)\n",
    "# );\n",
    "# \"\"\"\n",
    "\n",
    "# # Execute SQL via JVM bridge\n",
    "# conn = SparkSession.getActiveSession()._sc._jvm.java.sql.DriverManager.getConnection(\n",
    "#     redshift_url,\n",
    "#     redshift_properties[\"user\"],\n",
    "#     redshift_properties[\"password\"]\n",
    "# )\n",
    "# stmt = conn.createStatement()\n",
    "# stmt.execute(create_schema_sql)\n",
    "# stmt.execute(create_table_sql)\n",
    "# stmt.close()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51f900d2-9db2-4353-b318-9c791b7f8c69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe\n",
    "df_taxi_gold_updated.write \\\n",
    "  .mode(\"append\") \\\n",
    "  .jdbc(url=redshift_url, table=redshift_table, properties=redshift_properties)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1120258122580393,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Transform and load data for multiple months",
   "widgets": {
    "month": {
     "currentValue": "09",
     "nuid": "1b24bb3c-bc1a-46d2-9fc7-ef91f9c71b06",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter Month (e.g. 07)",
      "name": "month",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "Enter Month (e.g. 07)",
      "name": "month",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "year": {
     "currentValue": "2025",
     "nuid": "baf214ac-34f0-440e-a36d-4de7e43e28d9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter Year (e.g. 2025)",
      "name": "year",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "Enter Year (e.g. 2025)",
      "name": "year",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
