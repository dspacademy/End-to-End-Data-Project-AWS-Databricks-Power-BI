{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbfdb38e-d4fc-4187-89fa-e7867988872a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üöÄ Mount AWS S3 Bucket to Databricks File System (DBFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d8f3eeb-e283-4370-aef1-c725377ba4e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.parse  # Used to safely encode the secret key for URL usage\n",
    "\n",
    "# AWS credentials (use secrets management in production for security)\n",
    "ACCESS_KEY = \"<add access key>\"\n",
    "SECRET_KEY = \"<add secret key>\"\n",
    "\n",
    "# URL-encode the secret key to handle special characters\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(SECRET_KEY, safe='')\n",
    "\n",
    "# Define the S3 bucket name and the DBFS mount point\n",
    "aws_bucket_name = \"taxi-weather-analytics-s3-bucket\"\n",
    "mnt_name = \"/mnt\"\n",
    "\n",
    "# Mount the S3 bucket to DBFS using the s3a protocol and regional endpoint\n",
    "dbutils.fs.mount(\n",
    "  source = f\"s3a://{ACCESS_KEY}:{ENCODED_SECRET_KEY}@{aws_bucket_name}\",  # Authenticated S3 URI\n",
    "  mount_point = mnt_name,  # Local mount point in DBFS\n",
    "  extra_configs = {\n",
    "    \"fs.s3a.endpoint\": \"s3.ca-central-1.amazonaws.com\"  # Specify AWS region endpoint\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09afa20c-23d9-4529-a967-1d8e7fedf646",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"path\":215},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761665228971}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List files inside your mounted bucket path\n",
    "display(dbutils.fs.ls(\"/mnt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb0eb085-644a-4560-a484-a43d96c10e51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Deal with NYC taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2349dab-03de-47bb-bf3b-88d979503639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üõ†Ô∏è Load NYC Taxi Data and Register Bronze Delta Table in Hive Metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b6ef4c-5fc2-4ccf-9ce6-d5f9ac540b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read July 2025 yellow taxi trip data from the mounted S3 landing zone\n",
    "df_taxi = spark.read.parquet(f\"{mnt_name}/landing/nyc/taxi/2025/07/yellow_tripdata_2025-07.parquet\")\n",
    "\n",
    "# Create the bronze_nyc schema in the Hive metastore if it doesn't already exist\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS hive_metastore.bronze_nyc\")\n",
    "\n",
    "# Define the target path for saving the Delta table in the bronze zone\n",
    "target_path = \"/mnt/bronze/nyc/taxi/2025/07/daily_trip_2025-07\"\n",
    "\n",
    "# Write the DataFrame as a Delta table to the bronze path, overwriting if it exists\n",
    "df_taxi.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(target_path)\n",
    "\n",
    "# Register the Delta table in the Hive metastore for SQL access\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS hive_metastore.bronze_nyc.daily_trip\n",
    "    USING DELTA\n",
    "    LOCATION '{target_path}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3365b3d-e6fd-46a3-929c-ddf5d28d3032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM hive_metastore.bronze_nyc.daily_trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7fe1a4-b51e-4a0e-8957-5419c782b63d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- DROP SCHEMA hive_metastore.bronze_nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f3885d4-48fa-4e32-8bda-4f5b9d81530d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- üìä Query all records from the bronze-level NYC daily taxi trip Delta table\n",
    "SELECT * FROM hive_metastore.bronze_nyc.daily_trip;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a431677f-affb-46aa-bf27-0d1f9c06ee8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚ú® Clean and Standardize NYC Taxi Data for Silver Layer Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba5c3e6b-97d9-40be-afe4-944084d502d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, to_date\n",
    "\n",
    "# Apply type casting and timestamp normalization to prepare data for analytics and downstream modeling\n",
    "taxi_silver = (\n",
    "    df_taxi\n",
    "    .withColumn(\"tpep_pickup_datetime\", to_timestamp(\"tpep_pickup_datetime\"))  # Convert pickup time to timestamp\n",
    "    .withColumn(\"tpep_dropoff_datetime\", to_timestamp(\"tpep_dropoff_datetime\"))  # Convert dropoff time to timestamp\n",
    "    .withColumn(\"passenger_count\", col(\"passenger_count\").cast(\"int\"))  # Ensure passenger count is integer\n",
    "    .withColumn(\"trip_distance\", col(\"trip_distance\").cast(\"double\"))  # Cast trip distance to double\n",
    "    .withColumn(\"fare_amount\", col(\"fare_amount\").cast(\"double\"))  # Cast fare amount to double\n",
    "    .withColumn(\"tip_amount\", col(\"tip_amount\").cast(\"double\"))  # Cast tip amount to double\n",
    "    .withColumn(\"total_amount\", col(\"total_amount\").cast(\"double\"))  # Cast total amount to double\n",
    "    .withColumn(\"date\", to_date(col(\"tpep_pickup_datetime\")))  # Extract date for partitioning or filtering\n",
    "    .withColumn(\"pickup_datetime\", col(\"tpep_pickup_datetime\").cast(\"timestamp\"))  # Duplicate pickup time for clarity\n",
    ")\n",
    "\n",
    "# Display the transformed silver-level DataFrame for inspection\n",
    "display(taxi_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81aeecae-4a4f-429b-b1fe-2da17c3652e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check count before cleaning\n",
    "taxi_silver.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6370a836-bdea-48b4-b221-21cfbef0c4cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean taxi data\n",
    "df_taxi_silver_clean = taxi_silver \\\n",
    "  .dropDuplicates()\n",
    "# Check count after removing duplicate\n",
    "df_taxi_silver_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dced23f-c763-4a9b-a875-251bc70b96c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the cleaned taxi data\n",
    "display(df_taxi_silver_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1d975b9-2dc9-4c06-be82-829bee02563d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üßº Persist Cleaned NYC Taxi Data to Silver Layer and Register Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1d478e2-e5d4-49d6-a55b-619215f75e2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the silver_nyc schema in the Hive metastore if it doesn't already exist\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS hive_metastore.silver_nyc\")\n",
    "\n",
    "# Define the target path for saving the cleaned Delta table in the silver zone\n",
    "target_path = \"/mnt/silver/nyc/taxi/2025/07/cleaned_daily_trip_2025-07\"\n",
    "\n",
    "# Write the cleaned DataFrame to the silver path as a Delta table, overwriting if it exists\n",
    "df_taxi_silver_clean.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(target_path)\n",
    "\n",
    "# Register the cleaned Delta table in the Hive metastore for SQL access\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS hive_metastore.silver_nyc.cleaned_daily_trip\n",
    "    USING DELTA\n",
    "    LOCATION '{target_path}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c97382-a88c-4656-9f8b-dd6b5560580a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Display cleaned taxi data\n",
    "SELECT * FROM hive_metastore.silver_nyc.cleaned_daily_trip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a351da4c-3b44-4ce5-aa7f-fec260dbaa64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare silver taxi trip data for gold layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "108e3a86-70b2-409f-94bc-4b30eeb9d16d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count, sum\n",
    "\n",
    "# Taxi daily summary\n",
    "df_taxi_silver_agg = df_taxi_silver_clean \\\n",
    "    .groupBy(\"date\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"trip_count\"), \n",
    "        avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "        sum(\"total_amount\").alias(\"total_revenue\")\n",
    "    )\n",
    "\n",
    "display(df_taxi_silver_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c30b8fa0-7bea-4632-a1e1-2e5da0ff61a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deal with Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca593c0e-54a3-4bed-bc58-103064900ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# üìÇ List all top-level directories and files under the mounted S3 path (/mnt)\n",
    "display(dbutils.fs.ls(mnt_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ababdc6e-9bb9-45b5-bb38-757911b31511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingest weather data and persist the object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d3cd230-d11d-4b2f-b35f-04bed773afed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üå¶Ô∏è Define Nested Schema for Daily Weather JSON Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f298ed-f760-4bd2-b5c1-f21941b224f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    "\n",
    "# Create a structured schema to parse weather JSON data with nested fields and arrays\n",
    "w_schema = StructType([\n",
    "    StructField(\"latitude\", DoubleType(), True),  # Geographic latitude of the location\n",
    "    StructField(\"longitude\", DoubleType(), True),  # Geographic longitude of the location\n",
    "    StructField(\"generationtime_ms\", DoubleType(), True),  # Time taken to generate the response (ms)\n",
    "    StructField(\"utc_offset_seconds\", IntegerType(), True),  # UTC offset in seconds\n",
    "    StructField(\"timezone\", StringType(), True),  # Full timezone name (e.g., America/New_York)\n",
    "    StructField(\"timezone_abbreviation\", StringType(), True),  # Abbreviated timezone (e.g., EST)\n",
    "    StructField(\"elevation\", DoubleType(), True),  # Elevation of the location in meters\n",
    "\n",
    "    # Units for each daily weather metric (e.g., ¬∞C, mm)\n",
    "    StructField(\"daily_units\", StructType([\n",
    "        StructField(\"time\", StringType(), True),\n",
    "        StructField(\"temperature_2m_max\", StringType(), True),\n",
    "        StructField(\"temperature_2m_min\", StringType(), True),\n",
    "        StructField(\"precipitation_sum\", StringType(), True)\n",
    "    ]), True),\n",
    "\n",
    "    # Actual daily weather data as arrays\n",
    "    StructField(\"daily\", StructType([\n",
    "        StructField(\"time\", ArrayType(StringType()), True),  # Dates for each observation\n",
    "        StructField(\"temperature_2m_max\", ArrayType(DoubleType()), True),  # Daily max temperatures\n",
    "        StructField(\"temperature_2m_min\", ArrayType(DoubleType()), True),  # Daily min temperatures\n",
    "        StructField(\"precipitation_sum\", ArrayType(DoubleType()), True)  # Daily total precipitation\n",
    "    ]), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f50f24a1-c554-479a-94b7-75076acdc371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üå§Ô∏è Load Raw Daily Weather JSON into DataFrame Using Defined Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6cc693f-d403-4c64-a368-1cedaa9873dd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760223324340}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read weather data from the mounted S3 landing path using the nested schema for proper parsing\n",
    "weather_df = (\n",
    "    spark.read\n",
    "         .schema(w_schema)\n",
    "         .json(f\"{mnt_name}/landing/nyc/weather/2025/07/\")\n",
    ")\n",
    "\n",
    "# Display the parsed weather DataFrame for inspection and validation\n",
    "display(weather_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ce900f-42cf-4108-83e0-a1dbc0fbe6ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üåßÔ∏è Flatten and Categorize Daily Weather Data for Silver Layer Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e350296-51bb-44a1-bce5-12e3554d907b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, arrays_zip, col, when\n",
    "\n",
    "# Step 1: Zip the daily weather arrays into a single array of structs for row-wise expansion\n",
    "zipped_df = weather_df.select(\n",
    "    \"latitude\", \"longitude\", \"generationtime_ms\", \"utc_offset_seconds\",\n",
    "    \"timezone\", \"timezone_abbreviation\", \"elevation\",\n",
    "    col(\"daily_units.*\"),  # Flatten the daily_units struct into individual columns\n",
    "    explode(arrays_zip(  # Combine daily arrays into a single array of structs and explode into rows\n",
    "        col(\"daily.time\"),\n",
    "        col(\"daily.temperature_2m_max\"),\n",
    "        col(\"daily.temperature_2m_min\"),\n",
    "        col(\"daily.precipitation_sum\")\n",
    "    )).alias(\"daily\")  # Alias the exploded struct as 'daily'\n",
    ")\n",
    "\n",
    "# Step 2: Flatten the zipped struct into individual columns for analysis and joining\n",
    "bronze_flat_df = zipped_df.select(\n",
    "    \"latitude\", \"longitude\", \"generationtime_ms\", \"utc_offset_seconds\",\n",
    "    \"timezone\", \"timezone_abbreviation\", \"elevation\",\n",
    "    col(\"time\").alias(\"unit_time_format\"),  # Units metadata for time\n",
    "    col(\"temperature_2m_max\").alias(\"unit_temp_max\"),  # Units metadata for max temp\n",
    "    col(\"temperature_2m_min\").alias(\"unit_temp_min\"),  # Units metadata for min temp\n",
    "    col(\"precipitation_sum\").alias(\"unit_precip_mm\"),  # Units metadata for precipitation\n",
    "    col(\"daily.time\").alias(\"date\"),  # Actual date of observation\n",
    "    col(\"daily.temperature_2m_max\").alias(\"temperature_max\"),  # Max temperature value\n",
    "    col(\"daily.temperature_2m_min\").alias(\"temperature_min\"),  # Min temperature value\n",
    "    col(\"daily.precipitation_sum\").alias(\"precipitation_mm\")  # Precipitation value\n",
    ")\n",
    "\n",
    "# Step 3: Add a rainfall category column based on precipitation thresholds\n",
    "silver_cleaned_df = bronze_flat_df.withColumn(\n",
    "    \"rainfall_category\",\n",
    "    when(col(\"precipitation_mm\") < 2, \"Dry/No rain\")\n",
    "    .when(col(\"precipitation_mm\") < 10, \"Light Rain\")\n",
    "    .when(col(\"precipitation_mm\") < 30, \"Moderate Rain\")\n",
    "    .when(col(\"precipitation_mm\") < 60, \"Heavy Rain\")\n",
    "    .when(col(\"precipitation_mm\") < 100, \"Very Heavy Rain\")\n",
    "    .otherwise(\"Extreme Rainfall\")\n",
    ")\n",
    "\n",
    "# Display the cleaned and categorized silver-level weather DataFrame\n",
    "silver_cleaned_df.display(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "133e1070-f781-4a16-a7ba-99f6d3ac406c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üåü Enrich Aggregated Taxi Data with Daily Weather Metrics and Rainfall Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86ba17f0-6692-482a-8c87-2a706ed9da7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select relevant weather columns for joining with taxi data by date\n",
    "flat_df_selection = silver_cleaned_df.select(\n",
    "    \"date\",  # Date of weather observation\n",
    "    \"temperature_max\",  # Daily maximum temperature\n",
    "    \"temperature_min\",  # Daily minimum temperature\n",
    "    \"precipitation_mm\",  # Total daily precipitation in mm\n",
    "    \"rainfall_category\"  # Categorized rainfall intensity\n",
    ")\n",
    "\n",
    "# Perform a left join to enrich taxi aggregates with weather data based on matching date\n",
    "df_taxi_gold_updated = df_taxi_silver_agg.join(flat_df_selection, \"date\", \"left\")\n",
    "\n",
    "# Display the enriched gold-level DataFrame for inspection and downstream analysis\n",
    "display(df_taxi_gold_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4acc31b0-8691-4f82-9748-fb0e1560fd8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üèÜ Persist Final Gold-Level Taxi + Weather Dataset and Register Delta Table in Hive Metastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abaee3ec-9043-4930-9e23-0f8967860bc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üß© Reusable Function to Save DataFrame as Delta Table and Register in Hive Metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef41a17-9c54-4e2f-b040-dc95ff818826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_and_register_delta_table(df, target_path, schema_name, table_name):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame to the specified path as a Delta table and registers it in the Hive metastore.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The Spark DataFrame to persist.\n",
    "    - target_path (str): The DBFS or S3 path where the Delta table will be saved.\n",
    "    - schema_name (str): Hive metastore schema to create/use.\n",
    "    - table_name (str): Name of the table to register.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Create schema if it doesn't exist\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS hive_metastore.{schema_name}\")\n",
    "\n",
    "    # Write DataFrame as Delta table\n",
    "    df.write \\\n",
    "      .format(\"delta\") \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save(target_path)\n",
    "\n",
    "    # Register table in Hive metastore\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS hive_metastore.{schema_name}.{table_name}\n",
    "        USING DELTA\n",
    "        LOCATION '{target_path}'\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bf66a0a-dc7d-4e37-baa5-1f7e86e9bd50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_and_register_delta_table(\n",
    "    df=bronze_flat_df,\n",
    "    target_path=\"/mnt/bronze/nyc/weather/2025/07/daily_weather\",\n",
    "    schema_name=\"bronze_nyc\",\n",
    "    table_name=\"daily_weather\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb65761e-e21f-4589-9947-ad988558c60c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_and_register_delta_table(\n",
    "    df=silver_cleaned_df,\n",
    "    target_path=\"/mnt/silver/nyc/weather/2025/07/cleaned_daily_weather\",\n",
    "    schema_name=\"silver_nyc\",\n",
    "    table_name=\"cleaned_daily_weather\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71ece20a-c2ea-4d91-abc0-c486d05cd9ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_and_register_delta_table(\n",
    "    df=df_taxi_gold_updated,\n",
    "    target_path=\"/mnt/gold/nyc/trip_weather/2025/07/final_daily_trip_weather\",\n",
    "    schema_name=\"gold_nyc\",\n",
    "    table_name=\"final_daily_trip_weather\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bfd70f5-94b8-43d1-a96a-409befddb4a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_taxi_gold_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6212deb2-b607-4f7e-b574-7e5275fb633c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View final gold data\n",
    "SELECT * FROM hive_metastore.gold_nyc.final_daily_trip_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "294d0ca4-3e60-4159-be76-528de1680bd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Write data to Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bde5fc6-18a8-4f4b-90e5-7c7ede46c3f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üîó Test Redshift JDBC Connection from Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3bdbfd5-49e6-42d6-a684-3ba9d07e1267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # JDBC connection parameters\n",
    "# jdbc_url = \"jdbc:redshift://<add path>\"\n",
    "# user = \"<add user>\"\n",
    "# password = \"<add password>\"\n",
    "\n",
    "# # Use a lightweight system table for testing the connection\n",
    "# table_name = \"pg_catalog.pg_tables\"  # Redshift system catalog table\n",
    "\n",
    "# # -----------------------------------------------\n",
    "# # üöÄ Attempt to connect and read sample data\n",
    "# # -----------------------------------------------\n",
    "# try:\n",
    "#     # Read the system table using Spark JDBC\n",
    "#     test_df = spark.read \\\n",
    "#         .format(\"jdbc\") \\\n",
    "#         .option(\"url\", jdbc_url) \\\n",
    "#         .option(\"dbtable\", table_name) \\\n",
    "#         .option(\"user\", user) \\\n",
    "#         .option(\"password\", password) \\\n",
    "#         .option(\"driver\", \"com.amazon.redshift.jdbc42.Driver\") \\\n",
    "#         .load()\n",
    "\n",
    "#     # Display a few rows to confirm success\n",
    "#     print(\"‚úÖ Redshift connection successful. Sample rows:\")\n",
    "#     test_df.show(5)\n",
    "\n",
    "# except Exception as e:\n",
    "#     # Handle connection or read errors\n",
    "#     print(\"‚ùå Redshift connection failed.\")\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80a8056-e0b9-44f0-837a-651555caba9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# redshift_url = \"jdbc:redshift://<add path>\"\n",
    "# redshift_table = \"gold_nyc_2.final_daily_trip_weather\"\n",
    "# redshift_properties = {\n",
    "#     \"user\": \"<add user>\",\n",
    "#     \"password\": \"<add password>\",\n",
    "#     \"driver\": \"com.amazon.redshift.jdbc.Driver\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2614f17a-94d6-4ebe-8220-9cc9d11494dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe\n",
    "# df_taxi_gold_updated.write \\\n",
    "#   .mode(\"append\") \\\n",
    "#   .jdbc(url=redshift_url, table=redshift_table, properties=redshift_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00fba3b0-ddc3-4ade-ab7e-7d8ee5290f3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # table\n",
    "# df_table = spark.table(\"nyc_weather_table\")\n",
    "\n",
    "# df_table.write \\\n",
    "#   .mode(\"append\") \\\n",
    "#   .jdbc(url=redshift_url, table=redshift_table, properties=redshift_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7edc187-6864-4388-85e4-70035a9405da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Schema and Table via JDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b11c6c8d-8c19-4445-a700-43e6a3cf4588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # SQL to create a new database and schema\n",
    "# create_schema_sql = \"CREATE SCHEMA IF NOT EXISTS gold_nyc_2;\"\n",
    "# create_table_sql = \"\"\"\n",
    "# CREATE TABLE gold_nyc_2.final_daily_trip_weather (\n",
    "#     date DATE,\n",
    "#     trip_count BIGINT,\n",
    "#     avg_fare DOUBLE PRECISION,\n",
    "#     total_revenue DOUBLE PRECISION,\n",
    "#     temperature_max DOUBLE PRECISION,\n",
    "#     temperature_min DOUBLE PRECISION,\n",
    "#     precipitation_mm DOUBLE PRECISION,\n",
    "#     rainfall_category VARCHAR(50)\n",
    "# );\n",
    "# \"\"\"\n",
    "\n",
    "# # Execute SQL via JVM bridge\n",
    "# conn = SparkSession.getActiveSession()._sc._jvm.java.sql.DriverManager.getConnection(\n",
    "#     redshift_url,\n",
    "#     redshift_properties[\"user\"],\n",
    "#     redshift_properties[\"password\"]\n",
    "# )\n",
    "# stmt = conn.createStatement()\n",
    "# stmt.execute(create_schema_sql)\n",
    "# stmt.execute(create_table_sql)\n",
    "# stmt.close()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91699b5c-6337-42e6-a581-55f44131bff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "redshift_url = \"jdbc:redshift://<add path>\"\n",
    "redshift_table = \"gold_nyc.final_daily_trip_weather\"\n",
    "redshift_properties = {\n",
    "    \"user\": \"<add user>\",\n",
    "    \"password\": \"<add password>\",\n",
    "    \"driver\": \"com.amazon.redshift.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e324d775-be15-4c09-b84e-06e8c81e3685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# # JDBC connection to Redshift's default database (e.g., dev)\n",
    "# redshift_url = \"jdbc:redshift://<add path>\"\n",
    "# redshift_properties = {\n",
    "#     \"user\": \"<add user>\",\n",
    "#     \"password\": \"<add password\",\n",
    "#     \"driver\": \"com.amazon.redshift.jdbc.Driver\"\n",
    "# }\n",
    "\n",
    "# SQL to create a new database and schema\n",
    "create_schema_sql = \"CREATE SCHEMA IF NOT EXISTS gold_nyc;\"\n",
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE gold_nyc.final_daily_trip_weather (\n",
    "    date DATE,\n",
    "    trip_count BIGINT,\n",
    "    avg_fare DOUBLE PRECISION,\n",
    "    total_revenue DOUBLE PRECISION,\n",
    "    temperature_max DOUBLE PRECISION,\n",
    "    temperature_min DOUBLE PRECISION,\n",
    "    precipitation_mm DOUBLE PRECISION,\n",
    "    rainfall_category VARCHAR(50)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute SQL via JVM bridge\n",
    "conn = SparkSession.getActiveSession()._sc._jvm.java.sql.DriverManager.getConnection(\n",
    "    redshift_url,\n",
    "    redshift_properties[\"user\"],\n",
    "    redshift_properties[\"password\"]\n",
    ")\n",
    "stmt = conn.createStatement()\n",
    "stmt.execute(create_schema_sql)\n",
    "stmt.execute(create_table_sql)\n",
    "stmt.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51f900d2-9db2-4353-b318-9c791b7f8c69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe\n",
    "df_taxi_gold_updated.write \\\n",
    "  .mode(\"append\") \\\n",
    "  .jdbc(url=redshift_url, table=redshift_table, properties=redshift_properties)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4802581997944960,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Transform and load data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
